{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"colab":{"name":"DL_S3Pr10code.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"_uuid":"986ce81647d2c1a554e91d05692d30b593a144a4","id":"arR0b84hKUJ8","colab_type":"text"},"source":["## Importing dependencies"]},{"cell_type":"code","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","scrolled":true,"id":"hDC8iMrsKUJ9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1592731907418,"user_tz":-330,"elapsed":2027,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"617209c7-39e3-4af4-8196-7da1de437d4d"},"source":["import numpy as np\n","import pandas as pd\n","\n","import unicodedata, re, string\n","import nltk\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import TensorDataset, DataLoader\n","\n","import seaborn as sns\n","sns.set(color_codes=True)\n","\n","import matplotlib.pyplot as plt\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"_uuid":"3ea4268a41e63dbf967bdaf1d8a66112e0e295de","id":"vWC66JGOKUKC","colab_type":"text"},"source":["## Exploring The  Data"]},{"cell_type":"code","metadata":{"_uuid":"81953f0026a9fd8fc234e76b9dc225335efbc354","id":"5qpD-cqhKUKD","colab_type":"code","colab":{}},"source":["df_train = pd.read_csv(\"drive/My Drive/Datasets/DL_S3Pr10TrData.tsv\", sep=\"\\t\")\n","df_test = pd.read_csv(\"drive/My Drive/Datasets/DL_S3Pr10TeData.tsv\", sep=\"\\t\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"f69cc2ba0f2cb2ab79807a16b4d252b7a4339d6e","id":"ON-mwQMuKUKF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":202},"executionInfo":{"status":"ok","timestamp":1592664450151,"user_tz":-330,"elapsed":912,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"6a36a9ae-673b-4268-d348-e6899d6f07e7"},"source":["df_train.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 156060 entries, 0 to 156059\n","Data columns (total 4 columns):\n"," #   Column      Non-Null Count   Dtype \n","---  ------      --------------   ----- \n"," 0   PhraseId    156060 non-null  int64 \n"," 1   SentenceId  156060 non-null  int64 \n"," 2   Phrase      156060 non-null  object\n"," 3   Sentiment   156060 non-null  int64 \n","dtypes: int64(3), object(1)\n","memory usage: 4.8+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"109a0161dc81056c529f6f10cb558fd4e06a8206","id":"LSfGBsACKUKI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1592664450874,"user_tz":-330,"elapsed":1195,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"1b11eb1b-7959-4ccb-a05c-b807917485e7"},"source":["df_train.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>A series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>A</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>series</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   PhraseId  ...  Sentiment\n","0         1  ...          1\n","1         2  ...          2\n","2         3  ...          2\n","3         4  ...          2\n","4         5  ...          2\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"_uuid":"30d1ae62f1dc4f3decfd1fea4eca717c2c1ff288","id":"jukoem9FKUKM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1592664450874,"user_tz":-330,"elapsed":842,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"e8fbc024-f3cf-4959-9214-191c1f7f6faa"},"source":["df_train['Phrase'][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"_uuid":"bad32939bedaef058d0ecb0a0fae2ec5adee250e","id":"NDl06Q3nKUKQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"executionInfo":{"status":"ok","timestamp":1592664451538,"user_tz":-330,"elapsed":1025,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"f0e79886-5015-4dfc-b407-08c4e07e3891"},"source":["df_train.loc[df_train['SentenceId'] == 1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>PhraseId</th>\n","      <th>SentenceId</th>\n","      <th>Phrase</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>A series of escapades demonstrating the adage ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>A series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>A</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>series</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>58</th>\n","      <td>59</td>\n","      <td>1</td>\n","      <td>much</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>59</th>\n","      <td>60</td>\n","      <td>1</td>\n","      <td>of a story</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>61</td>\n","      <td>1</td>\n","      <td>a story</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>62</td>\n","      <td>1</td>\n","      <td>story</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>63</td>\n","      <td>1</td>\n","      <td>.</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>63 rows Ã— 4 columns</p>\n","</div>"],"text/plain":["    PhraseId  ...  Sentiment\n","0          1  ...          1\n","1          2  ...          2\n","2          3  ...          2\n","3          4  ...          2\n","4          5  ...          2\n","..       ...  ...        ...\n","58        59  ...          2\n","59        60  ...          2\n","60        61  ...          2\n","61        62  ...          2\n","62        63  ...          2\n","\n","[63 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"_uuid":"d6b71ead238ca90002183a854334da49dd319360","id":"xjNo2kriKUKT","colab_type":"text"},"source":["As was mentioned in the original competition description, there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment category. The competition is evaluated based on scoring results of each test phrase, so the context of the whole review does not matter here. The data is also fairly clean, so there will not be need for much pre-processing.\n","Before proceeding it is also a good idea to look at distribution of data, to see if the classes in training set are evenly distributed. For that I borrowed code from another Kaggle kernel:"]},{"cell_type":"code","metadata":{"_uuid":"14a584929ac8ef23a3bdf184d8fb1f65556fac38","id":"LPH9uKCQKUKU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":502},"executionInfo":{"status":"ok","timestamp":1592664453098,"user_tz":-330,"elapsed":1739,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"bae3485e-9811-4bea-ee19-8571205e4db6"},"source":["dist = df_train.groupby([\"Sentiment\"]).size()\n","\n","fig, ax = plt.subplots(figsize=(12,8))\n","sns.barplot(dist.keys(), dist.values);"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 864x576 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"_uuid":"3485562c7cd2ac11917b5e9953b761ad94110a2e","id":"acVu4yG5KUKY","colab_type":"text"},"source":["Classes seem to follow a normal distribution, with most frequently distributed class being \"2\".  This could lead to model not having sufficient data to learn the less-represented classes. This is something to be aware of when evaluating the model."]},{"cell_type":"markdown","metadata":{"_uuid":"5ff180b541a1e811cec7086397097281bca17b10","id":"lSeVx8GkKUKZ","colab_type":"text"},"source":["## Pre-Processing\n","\n","Words need to be tokenized into numeric format to be passed to RNN. Before that, however, I will also filter out spaces and punctuation, and use lemmatization to further reduce dimensionality. At this moment I do not want to filter out \"stop-words\", as RNN's are good at learning context from previously encountered information. In case of movie reviews, phrase \"this movie is shit\" has opposite meaning of \"this movie is the shit\", so I want that information to be available to the model.\n","\n","Below are some helper functions I borrowed online to help prepare the data:"]},{"cell_type":"code","metadata":{"_uuid":"271abb29213dabf44a4ac92038346dd00f2b83ec","id":"FGeQnIHjKUKZ","colab_type":"code","colab":{}},"source":["def remove_non_ascii(words):\n","    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n","        new_words.append(new_word)\n","    return new_words\n","\n","def to_lowercase(words):\n","    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = word.lower()\n","        new_words.append(new_word)\n","    return new_words\n","\n","def remove_punctuation(words):\n","    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = re.sub(r'[^\\w\\s]', '', word)\n","        if new_word != '':\n","            new_words.append(new_word)\n","    return new_words\n","\n","def remove_numbers(words):\n","    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n","    new_words = []\n","    for word in words:\n","        new_word = re.sub(\"\\d+\", \"\", word)\n","        if new_word != '':\n","            new_words.append(new_word)\n","    return new_words\n","\n","def remove_stopwords(words):\n","    \"\"\"Remove stop words from list of tokenized words\"\"\"\n","    new_words = []\n","    for word in words:\n","        if word not in stopwords.words('english'):\n","            new_words.append(word)\n","    return new_words\n","\n","def stem_words(words):\n","    \"\"\"Stem words in list of tokenized words\"\"\"\n","    stemmer = LancasterStemmer()\n","    stems = []\n","    for word in words:\n","        stem = stemmer.stem(word)\n","        stems.append(stem)\n","    return stems\n","\n","def lemmatize_verbs(words):\n","    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n","    lemmatizer = WordNetLemmatizer()\n","    lemmas = []\n","    for word in words:\n","        lemma = lemmatizer.lemmatize(word, pos='v')\n","        lemmas.append(lemma)\n","    return lemmas\n","\n","def normalize(words):\n","    words = remove_non_ascii(words)\n","    words = to_lowercase(words)\n","    words = remove_punctuation(words)\n","    words = remove_numbers(words)\n","#    words = remove_stopwords(words)\n","    return words"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"c62b3a073970c68147125e5dc4761fab0b111413","id":"yLAxU5gtKUKb","colab_type":"text"},"source":["Time to get the hands dirty. First I will go through the dataframe and tokenize each word using NLTK. Then I will pass each token through the prepping functions I created earlier, with the end result being a reduced list of lemmatized word tokens:"]},{"cell_type":"code","metadata":{"_uuid":"0c7e2fa70145639438d6a17c0af8e8c61517768f","id":"6qHAFZSpKUKc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1592664476542,"user_tz":-330,"elapsed":19065,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"a97fa672-cb7e-43a4-e95e-48fac42bd546"},"source":["# First step - tokenizing phrases\n","df_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n","\n","# Second step - passing through prep functions\n","df_train['Words'] = df_train['Words'].apply(normalize) \n","df_train['Words'].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [a, series, of, escapades, demonstrating, the,...\n","1    [a, series, of, escapades, demonstrating, the,...\n","2                                          [a, series]\n","3                                                  [a]\n","4                                             [series]\n","Name: Words, dtype: object"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"_uuid":"157930b980a9f30179276767d9b44bdef8c76af8","id":"ik5YkzRmKUKf","colab_type":"text"},"source":["Looks ok. Now the next prep step - converting words to number representations, as the embedding lookup requires that integers are passed to the network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Using this vocab then words in each phrase can be converted to integers:"]},{"cell_type":"code","metadata":{"_uuid":"d6ed075962d2b8cabb2193803d7cd5e8ea7b8ca4","id":"g-XkrKo-KUKg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1592664480834,"user_tz":-330,"elapsed":1239,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"7ab698b1-e18a-4b40-99e6-6a5446a58cb9"},"source":["# Third step - creating a list of unique words to be used as dictionary for encoding\n","word_set = set()\n","for l in df_train['Words']:\n","    for e in l:\n","        word_set.add(e)\n","        \n","word_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n","\n","# Check if they are still the same lenght\n","print(len(word_set))\n","print(len(word_to_int))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["16209\n","16209\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"7a41485c79f55a5981b07b77abc67cf2e2046ee7","id":"N41rdF9AKUKk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1592664483008,"user_tz":-330,"elapsed":1271,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"c9be3ebc-1770-4e5e-9b82-ccb0ed674dda"},"source":["# Now the dict to tokenize each phrase\n","df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n","df_train['Tokens'].head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0    [4419, 12839, 13362, 7894, 8426, 4067, 4911, 1...\n","1    [4419, 12839, 13362, 7894, 8426, 4067, 4911, 1...\n","2                                        [4419, 12839]\n","3                                               [4419]\n","4                                              [12839]\n","Name: Tokens, dtype: object"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"_uuid":"40ce0f1b2671f9b542f55b634a80996d7d80ebed","id":"lqmAI6IuKUKo","colab_type":"text"},"source":["So far so good. But for the input to the network the length of each phrase sequence has to be equal, so the shorter phrases will need to be \"padded\" - zeros added so that their token numbers are the same length."]},{"cell_type":"code","metadata":{"_uuid":"bd4c04ded25ff052cff54f3a3695d7e880229dca","id":"qW26PPP0KUKo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592664485938,"user_tz":-330,"elapsed":1190,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"2a23fbf7-5d6f-486d-875f-1a41ac9650fc"},"source":["# Step four - get the len of longest phrase\n","max_len = df_train['Tokens'].str.len().max()\n","print(max_len)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["48\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"380adf993084f5aec052f0816be116680f44e403","scrolled":true,"id":"WZcObF4eKUKq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"executionInfo":{"status":"ok","timestamp":1592664487887,"user_tz":-330,"elapsed":1259,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"3da4f6e1-a4cc-4fb9-9769-b7dd9ec493d5"},"source":["# Pad each phrase representation with zeroes, starting from the beginning of sequence\n","# Will use a combined list of phrases as np array for further work. This is expected format for the Pytorch utils to be used later\n","\n","all_tokens = np.array([t for t in df_train['Tokens']])\n","encoded_labels = np.array([l for l in df_train['Sentiment']])\n","\n","# Create blank rows\n","features = np.zeros((len(all_tokens), max_len), dtype=int)\n","# for each phrase, add zeros at the end \n","for i, row in enumerate(all_tokens):\n","    features[i, :len(row)] = row\n","\n","#print first 3 values of the feature matrix \n","print(features[:3])\n","\n"," "],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 4419 12839 13362  7894  8426  4067  4911 11317  9555 15880 10791  9176\n","   4067  6162 15880  8110 10791  9176  4067 13615 12456 13362  4913  9020\n","   6874  9174 15821 13362  4913  7847  2605  9516 13362  4419 10233     0\n","      0     0     0     0     0     0     0     0     0     0     0     0]\n"," [ 4419 12839 13362  7894  8426  4067  4911 11317  9555 15880 10791  9176\n","   4067  6162     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0]\n"," [ 4419 12839     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0\n","      0     0     0     0     0     0     0     0     0     0     0     0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"513a47fbb21155a850e0845839aec12962afe5f8","id":"VQbYMqycKUKt","colab_type":"text"},"source":["## Splitting the Data for Training, Validation, Test\n","\n","Time to split the data into training, validation, and test sets. For this purpose I will reserve 80% of training data for training, and remaining 20% will be split equally for validation and testing purposes.\n"]},{"cell_type":"code","metadata":{"_uuid":"9879d59cdaba3f64bafe7e1724251ee80dc3a16a","id":"waYHszmPKUKt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1592664491176,"user_tz":-330,"elapsed":1290,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"802d8f66-236c-441f-8cbf-ade7e39f839e"},"source":["split_frac = 0.8\n","\n","## split data into training, validation, and test data (features and labels, x and y)\n","\n","split_idx = int(len(features)*0.8)\n","train_x, remaining_x = features[:split_idx], features[split_idx:]\n","train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n","\n","test_idx = int(len(remaining_x)*0.5)\n","val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n","val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n","\n","## print out the shapes of  resultant feature data\n","print(\"\\t\\t\\tFeature Shapes:\")\n","print(\"Train set: \\t\\t{}\".format(train_x.shape), \n","      \"\\nValidation set: \\t{}\".format(val_x.shape),\n","      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\t\t\tFeature Shapes:\n","Train set: \t\t(124848, 48) \n","Validation set: \t(15606, 48) \n","Test set: \t\t(15606, 48)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"77cd3cba1b1a3546fe3f6903399f399ef217884a","id":"oggihktUKUKx","colab_type":"text"},"source":["## DataLoaders and Batching\n","After creating training, test, and validation data, time top create DataLoaders. They are the expected way to pass data into the model for training / testing. Loaders are created by following two steps:\n","\n","1) Create a known format for accessing data, using TensorDataset which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n","\n","2) Create DataLoaders and batch our training, validation, and test Tensor datasets."]},{"cell_type":"code","metadata":{"_uuid":"1f71a409ddc465da2108550c6260120b11db5a7f","id":"pCPx0ffIKUKx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1592664494018,"user_tz":-330,"elapsed":1210,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"e42866a8-b508-4886-94a5-c01c488da08a"},"source":["# create Tensor datasets\n","train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n","valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n","test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n","\n","# dataloaders\n","batch_size = 54\n","\n","# make sure the SHUFFLE your training data\n","train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n","valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n","test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n","\n","# Check the size of the loaders (how many batches inside)\n","print(len(train_loader))\n","print(len(valid_loader))\n","print(len(test_loader))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2312\n","289\n","289\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"d3621e913b85c6015d00b12b11adf5e0f68933a1","id":"OnTEiWhqKUK1","colab_type":"text"},"source":["## Creating a Deep Network\n","\n","The following text is borrowed from another excersize file, but it describes the approach of model building very well:\n","\n","1) First, we'll pass in words to an embedding layer. We need an embedding layer because we have thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.\n","\n","2) After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells. The LSTM cells will add recurrent connections to the network and give us the ability to include information about the sequence of words in the movie review data.\n","LSTM takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n","\n","3) Finally, the LSTM outputs will go to a linear layer for final classification, which outputs in turn will be passed to cross-entropy loss function to obtain probabilities for each predicted class.\n","\n","The layers are as follows:\n","* An embedding layer that converts word tokens (integers) into embeddings of a specific size.\n","* An LSTM layer defined by a hidden_state size and number of layers\n","* A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n","* A softmax will be applyed later by Crossentropy loss function, which turns all outputs into a probability\n","\n","Most of the time, the network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships.\n"]},{"cell_type":"code","metadata":{"_uuid":"4395f5c0690c68ea73bb0f0d8f78b453c95c49cd","id":"_O_0XnjfKUK3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1592664496739,"user_tz":-330,"elapsed":1328,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"31996ea2-a8f6-48d0-a5ea-a0774c4f791b"},"source":["# First checking if GPU is available\n","train_on_gpu=torch.cuda.is_available()\n","\n","if(train_on_gpu):\n","    print('Training on GPU.')\n","else:\n","    print('No GPU available, training on CPU.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["No GPU available, training on CPU.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"083cc4a1a4ea66e1c0188338b53a3f1ddecd1e24","id":"D84rGa35KUK8","colab_type":"code","colab":{}},"source":["class SentimentRNN(nn.Module):\n","    \"\"\"\n","    The RNN model that will be used to perform Sentiment analysis.\n","    \"\"\"\n","\n","    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n","        \"\"\"\n","        Initialize the model by setting up the layers.\n","        \"\"\"\n","        super(SentimentRNN, self).__init__()\n","\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.hidden_dim = hidden_dim\n","        \n","        # embedding and LSTM layers\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n","                            dropout=drop_prob, batch_first=True)\n","        \n","        # dropout layer\n","        self.dropout = nn.Dropout(0.3)\n","        \n","        # linear\n","        self.fc = nn.Linear(hidden_dim, output_size)\n","        \n","    def forward(self, x, hidden):\n","        \"\"\"\n","        Perform a forward pass of our model on some input and hidden state.\n","        \"\"\"\n","        batch_size = x.size(0)\n","\n","        # embeddings and lstm_out\n","        embeds = self.embedding(x)\n","\n","        lstm_out, hidden = self.lstm(embeds, hidden)\n","\n","        # transform lstm output to input size of linear layers\n","        lstm_out = lstm_out.transpose(0,1)\n","        lstm_out = lstm_out[-1]\n","\n","        out = self.dropout(lstm_out)\n","        out = self.fc(out)        \n","\n","        return out, hidden\n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n","                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n","        \n","        return hidden"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"_uuid":"6b3833c651a26de8c38d005a3566340e43d2a47b","id":"477LIy6HKULA","colab_type":"text"},"source":["## Instantiate the network\n","Here, we'll instantiate the network. First up, defining the hyperparameters.\n","* vocab_size: Size of our vocabulary or the range of values for our input, word tokens.\n","* output_size: Size of our desired output; the number of class scores we want to output (0..4).\n","* embedding_dim: Number of columns in the embedding lookup table; size of our embeddings.\n","* hidden_dim: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n","* n_layers: Number of LSTM layers in the network. Typically between 1-3"]},{"cell_type":"code","metadata":{"_uuid":"3e1ebcf1dcdf3ae074696b41538b4f593bfa525d","id":"TA29hr-UKULB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":118},"executionInfo":{"status":"ok","timestamp":1592664512072,"user_tz":-330,"elapsed":1243,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"111dd0cf-6880-4ea8-d206-a25ad5a929a9"},"source":["# Instantiate the model w/ hyperparams\n","vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n","output_size = 5\n","embedding_dim = 400\n","hidden_dim = 256\n","n_layers = 2\n","\n","net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n","\n","print(net)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SentimentRNN(\n","  (embedding): Embedding(16210, 400)\n","  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n","  (dropout): Dropout(p=0.3, inplace=False)\n","  (fc): Linear(in_features=256, out_features=5, bias=True)\n",")\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"024b493c908d9323330a1824369b2f9339372c33","id":"9TJmnYD2KULD","colab_type":"text"},"source":["## Training Routine\n","Below is the typical training code. \n","Crossentropy loss will be used, since this is multi-class classification problem.\n","We also have some data and training hyparameters:\n","* lr: Learning rate for our optimizer.\n","* epochs: Number of times to iterate through the training dataset.\n","* clip: The maximum gradient value to clip at (to prevent exploding gradients)."]},{"cell_type":"code","metadata":{"_uuid":"19f913418591679cf69d24df7b7e06b86b56a4be","id":"pF9H5N6CKULE","colab_type":"code","colab":{}},"source":["# loss and optimization functions\n","lr=0.003\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(net.parameters(), lr=lr)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"97778b9a79c598347e9f73712b50323d03405fef","id":"PJIfy8dTKULG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1592671399209,"user_tz":-330,"elapsed":6882336,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"50dff0d4-ba02-4d3c-9f61-4907de2db4a7"},"source":["# training params\n","epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n","\n","counter = 0\n","print_every = 100\n","clip=5 # gradient clipping\n","\n","# move model to GPU, if available\n","if(train_on_gpu):\n","    net.cuda()\n","\n","net.train()\n","# train for some number of epochs\n","for e in range(epochs):\n","    # initialize hidden state\n","    h = net.init_hidden(batch_size)\n","\n","    # batch loop\n","    for inputs, labels in train_loader:\n","        counter += 1\n","\n","        if(train_on_gpu):\n","            inputs, labels = inputs.cuda(), labels.cuda()\n","\n","        # Creating new variables for the hidden state, otherwise\n","        # we'd backprop through the entire training history\n","        h = tuple([each.data for each in h])\n","\n","        # zero accumulated gradients\n","        net.zero_grad()\n","\n","        # get the output from the model\n","        output, h = net(inputs, h)\n","        # calculate the loss and perform backprop\n","        loss = criterion(output, labels)\n","        loss.backward()\n","        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","        nn.utils.clip_grad_norm_(net.parameters(), clip)\n","        optimizer.step()\n","\n","        # loss stats\n","        if counter % print_every == 0:\n","            # Get validation loss\n","            val_h = net.init_hidden(batch_size)\n","            val_losses = []\n","            net.eval()\n","            for inputs, labels in valid_loader:\n","\n","                # Creating new variables for the hidden state, otherwise\n","                # we'd backprop through the entire training history\n","                val_h = tuple([each.data for each in val_h])\n","\n","                if(train_on_gpu):\n","                    inputs, labels = inputs.cuda(), labels.cuda()\n","\n","                output, val_h = net(inputs, val_h)\n","                val_loss = criterion(output, labels)\n","\n","                val_losses.append(val_loss.item())\n","\n","            net.train()\n","            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                  \"Step: {}...\".format(counter),\n","                  \"Loss: {:.6f}...\".format(loss.item()),\n","                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1/3... Step: 100... Loss: 1.112815... Val Loss: 1.303889\n","Epoch: 1/3... Step: 200... Loss: 1.345217... Val Loss: 1.305532\n","Epoch: 1/3... Step: 300... Loss: 1.363256... Val Loss: 1.302978\n","Epoch: 1/3... Step: 400... Loss: 1.201376... Val Loss: 1.303370\n","Epoch: 1/3... Step: 500... Loss: 1.310004... Val Loss: 1.300781\n","Epoch: 1/3... Step: 600... Loss: 1.426910... Val Loss: 1.316317\n","Epoch: 1/3... Step: 700... Loss: 1.409952... Val Loss: 1.304171\n","Epoch: 1/3... Step: 800... Loss: 1.235859... Val Loss: 1.304182\n","Epoch: 1/3... Step: 900... Loss: 1.172023... Val Loss: 1.307539\n","Epoch: 1/3... Step: 1000... Loss: 1.150193... Val Loss: 1.339248\n","Epoch: 1/3... Step: 1100... Loss: 1.132727... Val Loss: 1.296039\n","Epoch: 1/3... Step: 1200... Loss: 1.194905... Val Loss: 1.241045\n","Epoch: 1/3... Step: 1300... Loss: 0.935013... Val Loss: 1.233307\n","Epoch: 1/3... Step: 1400... Loss: 1.167408... Val Loss: 1.238919\n","Epoch: 1/3... Step: 1500... Loss: 1.386693... Val Loss: 1.241912\n","Epoch: 1/3... Step: 1600... Loss: 1.112416... Val Loss: 1.221919\n","Epoch: 1/3... Step: 1700... Loss: 1.118450... Val Loss: 1.204626\n","Epoch: 1/3... Step: 1800... Loss: 1.347797... Val Loss: 1.187824\n","Epoch: 1/3... Step: 1900... Loss: 1.067831... Val Loss: 1.182873\n","Epoch: 1/3... Step: 2000... Loss: 1.181157... Val Loss: 1.179462\n","Epoch: 1/3... Step: 2100... Loss: 1.034153... Val Loss: 1.182128\n","Epoch: 1/3... Step: 2200... Loss: 0.954356... Val Loss: 1.168539\n","Epoch: 1/3... Step: 2300... Loss: 1.178509... Val Loss: 1.178948\n","Epoch: 2/3... Step: 2400... Loss: 1.129460... Val Loss: 1.163036\n","Epoch: 2/3... Step: 2500... Loss: 1.225486... Val Loss: 1.172707\n","Epoch: 2/3... Step: 2600... Loss: 0.881210... Val Loss: 1.159708\n","Epoch: 2/3... Step: 2700... Loss: 1.052125... Val Loss: 1.153296\n","Epoch: 2/3... Step: 2800... Loss: 1.169031... Val Loss: 1.154510\n","Epoch: 2/3... Step: 2900... Loss: 0.928413... Val Loss: 1.177458\n","Epoch: 2/3... Step: 3000... Loss: 1.298545... Val Loss: 1.147508\n","Epoch: 2/3... Step: 3100... Loss: 1.037465... Val Loss: 1.162468\n","Epoch: 2/3... Step: 3200... Loss: 1.076119... Val Loss: 1.164705\n","Epoch: 2/3... Step: 3300... Loss: 1.006566... Val Loss: 1.146162\n","Epoch: 2/3... Step: 3400... Loss: 0.950265... Val Loss: 1.136527\n","Epoch: 2/3... Step: 3500... Loss: 1.167408... Val Loss: 1.133340\n","Epoch: 2/3... Step: 3600... Loss: 1.002332... Val Loss: 1.133198\n","Epoch: 2/3... Step: 3700... Loss: 0.966854... Val Loss: 1.146245\n","Epoch: 2/3... Step: 3800... Loss: 1.175847... Val Loss: 1.122770\n","Epoch: 2/3... Step: 3900... Loss: 1.103152... Val Loss: 1.125473\n","Epoch: 2/3... Step: 4000... Loss: 1.252192... Val Loss: 1.157440\n","Epoch: 2/3... Step: 4100... Loss: 0.896940... Val Loss: 1.126464\n","Epoch: 2/3... Step: 4200... Loss: 1.165152... Val Loss: 1.130837\n","Epoch: 2/3... Step: 4300... Loss: 0.848930... Val Loss: 1.118736\n","Epoch: 2/3... Step: 4400... Loss: 1.065927... Val Loss: 1.126230\n","Epoch: 2/3... Step: 4500... Loss: 0.930878... Val Loss: 1.117614\n","Epoch: 2/3... Step: 4600... Loss: 0.797803... Val Loss: 1.101683\n","Epoch: 3/3... Step: 4700... Loss: 1.013844... Val Loss: 1.121979\n","Epoch: 3/3... Step: 4800... Loss: 0.933471... Val Loss: 1.164182\n","Epoch: 3/3... Step: 4900... Loss: 1.158240... Val Loss: 1.124751\n","Epoch: 3/3... Step: 5000... Loss: 1.013741... Val Loss: 1.130397\n","Epoch: 3/3... Step: 5100... Loss: 0.988616... Val Loss: 1.174892\n","Epoch: 3/3... Step: 5200... Loss: 0.714723... Val Loss: 1.116897\n","Epoch: 3/3... Step: 5300... Loss: 1.076726... Val Loss: 1.099399\n","Epoch: 3/3... Step: 5400... Loss: 0.881262... Val Loss: 1.130757\n","Epoch: 3/3... Step: 5500... Loss: 0.823486... Val Loss: 1.145785\n","Epoch: 3/3... Step: 5600... Loss: 0.845862... Val Loss: 1.108549\n","Epoch: 3/3... Step: 5700... Loss: 0.931291... Val Loss: 1.120913\n","Epoch: 3/3... Step: 5800... Loss: 0.814661... Val Loss: 1.129973\n","Epoch: 3/3... Step: 5900... Loss: 1.024972... Val Loss: 1.116215\n","Epoch: 3/3... Step: 6000... Loss: 0.760528... Val Loss: 1.150951\n","Epoch: 3/3... Step: 6100... Loss: 0.675398... Val Loss: 1.110753\n","Epoch: 3/3... Step: 6200... Loss: 0.748943... Val Loss: 1.122417\n","Epoch: 3/3... Step: 6300... Loss: 0.743718... Val Loss: 1.071555\n","Epoch: 3/3... Step: 6400... Loss: 1.037930... Val Loss: 1.101362\n","Epoch: 3/3... Step: 6500... Loss: 0.741024... Val Loss: 1.113531\n","Epoch: 3/3... Step: 6600... Loss: 0.757766... Val Loss: 1.102893\n","Epoch: 3/3... Step: 6700... Loss: 0.799651... Val Loss: 1.079079\n","Epoch: 3/3... Step: 6800... Loss: 0.848688... Val Loss: 1.093274\n","Epoch: 3/3... Step: 6900... Loss: 1.001393... Val Loss: 1.119599\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"_uuid":"43750e21ea76ee1b90c0d3cffed952f68495e510","id":"27MJ1-ybKULI","colab_type":"text"},"source":["## Testing\n","\n","There are a few ways to test your network.\n","* Test data performance: First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n","* Inference on user-generated data: Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called inference.\n","\n","For the practical purposes of this example, though, second option is not applicable, as the task is to classify syntetic colection of provided phrases."]},{"cell_type":"code","metadata":{"_uuid":"73615beffd56464165901f7589bacc2dafa0d920","id":"OSjcroFzKULJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":50},"executionInfo":{"status":"ok","timestamp":1592671440415,"user_tz":-330,"elapsed":6100135,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"9d0132d3-c5b9-43e0-fbaf-4ae94eb4403b"},"source":["# Get test data loss and accuracy\n","\n","test_losses = [] # track loss\n","num_correct = 0\n","\n","# init hidden state\n","h = net.init_hidden(batch_size)\n","\n","net.eval()\n","# iterate over test data\n","for inputs, labels in test_loader:\n","\n","    # Creating new variables for the hidden state, otherwise\n","    # we'd backprop through the entire training history\n","    h = tuple([each.data for each in h])\n","\n","    if(train_on_gpu):\n","        inputs, labels = inputs.cuda(), labels.cuda()\n","    \n","    # get predicted outputs\n","    output, h = net(inputs, h)\n","    \n","    # calculate loss\n","    test_loss = criterion(output, labels)\n","    test_losses.append(test_loss.item())\n","    \n","    # convert output probabilities to predicted class\n","    _, pred = torch.max(output,1)\n","    \n","    # compare predictions to true label\n","    correct_tensor = pred.eq(labels.view_as(pred))\n","    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n","    num_correct += np.sum(correct)\n","\n","# -- stats! -- ##\n","# avg test loss\n","print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n","\n","# accuracy over all test data\n","test_acc = num_correct/len(test_loader.dataset)\n","print(\"Test accuracy: {:.3f}\".format(test_acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Test loss: 1.149\n","Test accuracy: 0.543\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"d61cc2aad4ddc8dafe532c04903c1c6931eb3564","id":"Z2rA84BmKULL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"executionInfo":{"status":"ok","timestamp":1592671491959,"user_tz":-330,"elapsed":1310,"user":{"displayName":"Gangadhar Katuri","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiICEasfuZknaE9S2AtYEhBn_BmCvRRKh4IQdQEVA=s64","userId":"02416538775544720796"}},"outputId":"032d3d27-8600-4cd1-baaa-8fba675bcc95"},"source":["pred"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1, 3, 2, 1, 2, 3, 2, 1, 4, 2, 2, 2, 2, 1, 2, 2, 3, 1, 1, 2, 2, 2, 2, 4,\n","        3, 2, 1, 2, 1, 1, 2, 2, 2, 1, 2, 1, 2, 1, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1,\n","        2, 4, 2, 2, 2, 1])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"code","metadata":{"id":"Sx_Xa7mDnaW9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}